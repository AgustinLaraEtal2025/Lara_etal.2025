2025-09-02 21:11:12 Training with configuration:
2025-09-02 21:11:12 data:
2025-09-02 21:11:12   bbox_margin: 20
2025-09-02 21:11:12   colormode: RGB
2025-09-02 21:11:12   inference:
2025-09-02 21:11:12     normalize_images: True
2025-09-02 21:11:12   train:
2025-09-02 21:11:12     affine:
2025-09-02 21:11:12       p: 0.5
2025-09-02 21:11:12       rotation: 30
2025-09-02 21:11:12       scaling: [0.5, 1.25]
2025-09-02 21:11:12       translation: 0
2025-09-02 21:11:12     crop_sampling:
2025-09-02 21:11:12       width: 448
2025-09-02 21:11:12       height: 448
2025-09-02 21:11:12       max_shift: 0.1
2025-09-02 21:11:12       method: hybrid
2025-09-02 21:11:12     gaussian_noise: 12.75
2025-09-02 21:11:12     motion_blur: True
2025-09-02 21:11:12     normalize_images: True
2025-09-02 21:11:12 device: auto
2025-09-02 21:11:12 metadata:
2025-09-02 21:11:12   project_path: C:\Users\agust\OneDrive\Desktop notebook casa\antenas-agustin-2025-09-02
2025-09-02 21:11:12   pose_config_path: C:\Users\agust\OneDrive\Desktop notebook casa\antenas-agustin-2025-09-02\dlc-models-pytorch\iteration-0\antenasSep2-trainset95shuffle2\train\pytorch_config.yaml
2025-09-02 21:11:12   bodyparts: ['al1', 'al2', 'al3', 'ar1', 'ar2', 'ar3', 'prob1', 'prob2', 'prob3', 'ml1', 'ml2', 'mr1', 'mr2']
2025-09-02 21:11:12   unique_bodyparts: []
2025-09-02 21:11:12   individuals: ['animal']
2025-09-02 21:11:12   with_identity: None
2025-09-02 21:11:12 method: bu
2025-09-02 21:11:12 model:
2025-09-02 21:11:12   backbone:
2025-09-02 21:11:12     type: ResNet
2025-09-02 21:11:12     model_name: resnet50_gn
2025-09-02 21:11:12     output_stride: 16
2025-09-02 21:11:12     freeze_bn_stats: False
2025-09-02 21:11:12     freeze_bn_weights: False
2025-09-02 21:11:12   backbone_output_channels: 2048
2025-09-02 21:11:12   heads:
2025-09-02 21:11:12     bodypart:
2025-09-02 21:11:12       type: HeatmapHead
2025-09-02 21:11:12       weight_init: normal
2025-09-02 21:11:12       predictor:
2025-09-02 21:11:12         type: HeatmapPredictor
2025-09-02 21:11:12         apply_sigmoid: False
2025-09-02 21:11:12         clip_scores: True
2025-09-02 21:11:12         location_refinement: True
2025-09-02 21:11:12         locref_std: 7.2801
2025-09-02 21:11:12       target_generator:
2025-09-02 21:11:12         type: HeatmapGaussianGenerator
2025-09-02 21:11:12         num_heatmaps: 13
2025-09-02 21:11:12         pos_dist_thresh: 17
2025-09-02 21:11:12         heatmap_mode: KEYPOINT
2025-09-02 21:11:12         gradient_masking: False
2025-09-02 21:11:12         generate_locref: True
2025-09-02 21:11:12         locref_std: 7.2801
2025-09-02 21:11:12       criterion:
2025-09-02 21:11:12         heatmap:
2025-09-02 21:11:12           type: WeightedMSECriterion
2025-09-02 21:11:12           weight: 1.0
2025-09-02 21:11:12         locref:
2025-09-02 21:11:12           type: WeightedHuberCriterion
2025-09-02 21:11:12           weight: 0.05
2025-09-02 21:11:12       heatmap_config:
2025-09-02 21:11:12         channels: [2048, 13]
2025-09-02 21:11:12         kernel_size: [3]
2025-09-02 21:11:12         strides: [2]
2025-09-02 21:11:12       locref_config:
2025-09-02 21:11:12         channels: [2048, 26]
2025-09-02 21:11:12         kernel_size: [3]
2025-09-02 21:11:12         strides: [2]
2025-09-02 21:11:12 net_type: resnet_50
2025-09-02 21:11:12 runner:
2025-09-02 21:11:12   type: PoseTrainingRunner
2025-09-02 21:11:12   gpus: None
2025-09-02 21:11:12   key_metric: test.mAP
2025-09-02 21:11:12   key_metric_asc: True
2025-09-02 21:11:12   eval_interval: 10
2025-09-02 21:11:12   optimizer:
2025-09-02 21:11:12     type: AdamW
2025-09-02 21:11:12     params:
2025-09-02 21:11:12       lr: 0.0005
2025-09-02 21:11:12   scheduler:
2025-09-02 21:11:12     type: LRListScheduler
2025-09-02 21:11:12     params:
2025-09-02 21:11:12       lr_list: [[0.0001], [1e-05]]
2025-09-02 21:11:12       milestones: [90, 120]
2025-09-02 21:11:12   snapshots:
2025-09-02 21:11:12     max_snapshots: 5
2025-09-02 21:11:12     save_epochs: 100
2025-09-02 21:11:12     save_optimizer_state: False
2025-09-02 21:11:12 train_settings:
2025-09-02 21:11:12   batch_size: 8
2025-09-02 21:11:12   dataloader_workers: 0
2025-09-02 21:11:12   dataloader_pin_memory: False
2025-09-02 21:11:12   display_iters: 100
2025-09-02 21:11:12   epochs: 500
2025-09-02 21:11:12   seed: 42
2025-09-02 21:11:12 Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)
2025-09-02 21:11:12 [timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-02 21:11:12 Data Transforms:
2025-09-02 21:11:12   Training:   Compose([
  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (0.5, 1.25), 'y': (0.5, 1.25)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),
  PadIfNeeded(always_apply=True, p=1.0, min_height=448, min_width=448, pad_height_divisor=None, pad_width_divisor=None, position=PositionType.CENTER, border_mode=0, value=None, mask_value=None),
  KeypointAwareCrop(always_apply=True, p=1.0, width=448, height=448, max_shift=0.1, crop_sampling='hybrid'),
  MotionBlur(always_apply=False, p=0.5, blur_limit=(3, 7), allow_shifted=True),
  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-09-02 21:11:12   Validation: Compose([
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-09-02 21:11:12 Using 30 images and 2 for testing
2025-09-02 21:11:12 
Starting pose model training...
--------------------------------------------------
2025-09-02 21:11:15 Epoch 1/500 (lr=0.0005), train loss 0.01757
2025-09-02 21:11:17 Epoch 2/500 (lr=0.0005), train loss 0.01481
2025-09-02 21:11:18 Epoch 3/500 (lr=0.0005), train loss 0.01288
2025-09-02 21:11:19 Epoch 4/500 (lr=0.0005), train loss 0.01166
2025-09-02 21:11:21 Epoch 5/500 (lr=0.0005), train loss 0.00987
2025-09-02 21:11:22 Epoch 6/500 (lr=0.0005), train loss 0.00889
2025-09-02 21:11:23 Epoch 7/500 (lr=0.0005), train loss 0.00794
2025-09-02 21:11:25 Epoch 8/500 (lr=0.0005), train loss 0.00721
2025-09-02 21:11:26 Epoch 9/500 (lr=0.0005), train loss 0.00607
2025-09-02 21:11:28 Training for epoch 10 done, starting evaluation
2025-09-02 21:11:28 Epoch 10/500 (lr=0.0005), train loss 0.00549, valid loss 0.00449
2025-09-02 21:11:28 Model performance:
2025-09-02 21:11:28   metrics/test.rmse:           8.62
2025-09-02 21:11:28   metrics/test.rmse_pcutoff:   5.77
2025-09-02 21:11:28   metrics/test.mAP:           85.05
2025-09-02 21:11:28   metrics/test.mAR:           85.00
2025-09-02 21:11:29 Epoch 11/500 (lr=0.0005), train loss 0.00536
2025-09-02 21:11:31 Epoch 12/500 (lr=0.0005), train loss 0.00531
2025-09-02 21:11:32 Epoch 13/500 (lr=0.0005), train loss 0.00520
2025-09-02 21:11:33 Epoch 14/500 (lr=0.0005), train loss 0.00456
2025-09-02 21:11:35 Epoch 15/500 (lr=0.0005), train loss 0.00440
2025-09-02 21:11:36 Epoch 16/500 (lr=0.0005), train loss 0.00408
2025-09-02 21:11:37 Epoch 17/500 (lr=0.0005), train loss 0.00411
2025-09-02 21:11:39 Epoch 18/500 (lr=0.0005), train loss 0.00356
2025-09-02 21:11:40 Epoch 19/500 (lr=0.0005), train loss 0.00391
2025-09-02 21:11:41 Training for epoch 20 done, starting evaluation
2025-09-02 21:11:42 Epoch 20/500 (lr=0.0005), train loss 0.00356, valid loss 0.00264
2025-09-02 21:11:42 Model performance:
2025-09-02 21:11:42   metrics/test.rmse:           3.83
2025-09-02 21:11:42   metrics/test.rmse_pcutoff:   3.95
2025-09-02 21:11:42   metrics/test.mAP:          100.00
2025-09-02 21:11:42   metrics/test.mAR:          100.00
2025-09-02 21:11:43 Epoch 21/500 (lr=0.0005), train loss 0.00346
2025-09-02 21:11:44 Epoch 22/500 (lr=0.0005), train loss 0.00311
2025-09-02 21:11:46 Epoch 23/500 (lr=0.0005), train loss 0.00299
2025-09-02 21:11:47 Epoch 24/500 (lr=0.0005), train loss 0.00308
2025-09-02 21:11:49 Epoch 25/500 (lr=0.0005), train loss 0.00283
2025-09-02 21:11:50 Epoch 26/500 (lr=0.0005), train loss 0.00294
2025-09-02 21:11:52 Epoch 27/500 (lr=0.0005), train loss 0.00282
2025-09-02 21:11:53 Epoch 28/500 (lr=0.0005), train loss 0.00248
2025-09-02 21:11:54 Epoch 29/500 (lr=0.0005), train loss 0.00226
2025-09-02 21:11:56 Training for epoch 30 done, starting evaluation
2025-09-02 21:11:56 Epoch 30/500 (lr=0.0005), train loss 0.00244, valid loss 0.00169
2025-09-02 21:11:56 Model performance:
2025-09-02 21:11:56   metrics/test.rmse:           3.41
2025-09-02 21:11:56   metrics/test.rmse_pcutoff:   3.41
2025-09-02 21:11:56   metrics/test.mAP:          100.00
2025-09-02 21:11:56   metrics/test.mAR:          100.00
2025-09-02 21:11:57 Epoch 31/500 (lr=0.0005), train loss 0.00218
2025-09-02 21:11:58 Epoch 32/500 (lr=0.0005), train loss 0.00235
2025-09-02 21:12:00 Epoch 33/500 (lr=0.0005), train loss 0.00240
2025-09-02 21:12:01 Epoch 34/500 (lr=0.0005), train loss 0.00244
2025-09-02 21:12:03 Epoch 35/500 (lr=0.0005), train loss 0.00228
2025-09-02 21:12:04 Epoch 36/500 (lr=0.0005), train loss 0.00250
2025-09-02 21:12:05 Epoch 37/500 (lr=0.0005), train loss 0.00230
2025-09-02 21:12:07 Epoch 38/500 (lr=0.0005), train loss 0.00219
2025-09-02 21:12:08 Epoch 39/500 (lr=0.0005), train loss 0.00212
2025-09-02 21:12:10 Training for epoch 40 done, starting evaluation
2025-09-02 21:12:10 Epoch 40/500 (lr=0.0005), train loss 0.00229, valid loss 0.00202
2025-09-02 21:12:10 Model performance:
2025-09-02 21:12:10   metrics/test.rmse:           3.89
2025-09-02 21:12:10   metrics/test.rmse_pcutoff:   3.89
2025-09-02 21:12:10   metrics/test.mAP:          100.00
2025-09-02 21:12:10   metrics/test.mAR:          100.00
2025-09-02 21:12:11 Epoch 41/500 (lr=0.0005), train loss 0.00256
2025-09-02 21:12:13 Epoch 42/500 (lr=0.0005), train loss 0.00237
2025-09-02 21:12:14 Epoch 43/500 (lr=0.0005), train loss 0.00216
2025-09-02 21:12:15 Epoch 44/500 (lr=0.0005), train loss 0.00219
2025-09-02 21:12:17 Epoch 45/500 (lr=0.0005), train loss 0.00206
2025-09-02 21:12:18 Epoch 46/500 (lr=0.0005), train loss 0.00225
2025-09-02 21:12:20 Epoch 47/500 (lr=0.0005), train loss 0.00212
2025-09-02 21:12:21 Epoch 48/500 (lr=0.0005), train loss 0.00187
2025-09-02 21:12:23 Epoch 49/500 (lr=0.0005), train loss 0.00219
2025-09-02 21:12:24 Training for epoch 50 done, starting evaluation
2025-09-02 21:12:24 Epoch 50/500 (lr=0.0005), train loss 0.00215, valid loss 0.00168
2025-09-02 21:12:24 Model performance:
2025-09-02 21:12:24   metrics/test.rmse:           2.87
2025-09-02 21:12:24   metrics/test.rmse_pcutoff:   2.87
2025-09-02 21:12:24   metrics/test.mAP:          100.00
2025-09-02 21:12:24   metrics/test.mAR:          100.00
2025-09-02 21:12:26 Epoch 51/500 (lr=0.0005), train loss 0.00189
2025-09-02 21:12:27 Epoch 52/500 (lr=0.0005), train loss 0.00184
2025-09-02 21:12:29 Epoch 53/500 (lr=0.0005), train loss 0.00188
2025-09-02 21:12:30 Epoch 54/500 (lr=0.0005), train loss 0.00180
2025-09-02 21:12:32 Epoch 55/500 (lr=0.0005), train loss 0.00188
2025-09-02 21:12:33 Epoch 56/500 (lr=0.0005), train loss 0.00215
2025-09-02 21:12:35 Epoch 57/500 (lr=0.0005), train loss 0.00222
2025-09-02 21:12:36 Epoch 58/500 (lr=0.0005), train loss 0.00154
2025-09-02 21:12:37 Epoch 59/500 (lr=0.0005), train loss 0.00165
2025-09-02 21:12:39 Training for epoch 60 done, starting evaluation
2025-09-02 21:12:39 Epoch 60/500 (lr=0.0005), train loss 0.00154, valid loss 0.00138
2025-09-02 21:12:39 Model performance:
2025-09-02 21:12:39   metrics/test.rmse:           2.66
2025-09-02 21:12:39   metrics/test.rmse_pcutoff:   2.66
2025-09-02 21:12:39   metrics/test.mAP:          100.00
2025-09-02 21:12:39   metrics/test.mAR:          100.00
2025-09-02 21:12:41 Epoch 61/500 (lr=0.0005), train loss 0.00152
2025-09-02 21:12:42 Epoch 62/500 (lr=0.0005), train loss 0.00148
2025-09-02 21:12:43 Epoch 63/500 (lr=0.0005), train loss 0.00134
2025-09-02 21:12:45 Epoch 64/500 (lr=0.0005), train loss 0.00165
2025-09-02 21:12:46 Epoch 65/500 (lr=0.0005), train loss 0.00158
2025-09-02 21:12:48 Epoch 66/500 (lr=0.0005), train loss 0.00131
2025-09-02 21:12:49 Epoch 67/500 (lr=0.0005), train loss 0.00127
2025-09-02 21:12:50 Epoch 68/500 (lr=0.0005), train loss 0.00143
2025-09-02 21:12:52 Epoch 69/500 (lr=0.0005), train loss 0.00158
2025-09-02 21:12:53 Training for epoch 70 done, starting evaluation
2025-09-02 21:12:53 Epoch 70/500 (lr=0.0005), train loss 0.00156, valid loss 0.00209
2025-09-02 21:12:53 Model performance:
2025-09-02 21:12:53   metrics/test.rmse:           3.71
2025-09-02 21:12:53   metrics/test.rmse_pcutoff:   3.71
2025-09-02 21:12:53   metrics/test.mAP:          100.00
2025-09-02 21:12:53   metrics/test.mAR:          100.00
2025-09-02 21:12:55 Epoch 71/500 (lr=0.0005), train loss 0.00146
2025-09-02 21:12:56 Epoch 72/500 (lr=0.0005), train loss 0.00136
2025-09-02 21:12:58 Epoch 73/500 (lr=0.0005), train loss 0.00144
2025-09-02 21:12:59 Epoch 74/500 (lr=0.0005), train loss 0.00129
2025-09-02 21:13:01 Epoch 75/500 (lr=0.0005), train loss 0.00136
2025-09-02 21:13:02 Epoch 76/500 (lr=0.0005), train loss 0.00138
2025-09-02 21:13:03 Epoch 77/500 (lr=0.0005), train loss 0.00132
2025-09-02 21:13:05 Epoch 78/500 (lr=0.0005), train loss 0.00154
2025-09-02 21:13:06 Epoch 79/500 (lr=0.0005), train loss 0.00128
2025-09-02 21:13:08 Training for epoch 80 done, starting evaluation
2025-09-02 21:13:08 Epoch 80/500 (lr=0.0005), train loss 0.00146, valid loss 0.00189
2025-09-02 21:13:08 Model performance:
2025-09-02 21:13:08   metrics/test.rmse:           3.55
2025-09-02 21:13:08   metrics/test.rmse_pcutoff:   3.55
2025-09-02 21:13:08   metrics/test.mAP:          100.00
2025-09-02 21:13:08   metrics/test.mAR:          100.00
2025-09-02 21:13:09 Epoch 81/500 (lr=0.0005), train loss 0.00144
2025-09-02 21:13:11 Epoch 82/500 (lr=0.0005), train loss 0.00144
2025-09-02 21:13:12 Epoch 83/500 (lr=0.0005), train loss 0.00132
2025-09-02 21:13:13 Epoch 84/500 (lr=0.0005), train loss 0.00115
2025-09-02 21:13:15 Epoch 85/500 (lr=0.0005), train loss 0.00118
2025-09-02 21:13:16 Epoch 86/500 (lr=0.0005), train loss 0.00136
2025-09-02 21:13:18 Epoch 87/500 (lr=0.0005), train loss 0.00138
2025-09-02 21:13:19 Epoch 88/500 (lr=0.0005), train loss 0.00118
2025-09-02 21:13:21 Epoch 89/500 (lr=0.0005), train loss 0.00137
2025-09-02 21:13:22 Training for epoch 90 done, starting evaluation
2025-09-02 21:13:22 Epoch 90/500 (lr=0.0001), train loss 0.00111, valid loss 0.00152
2025-09-02 21:13:22 Model performance:
2025-09-02 21:13:22   metrics/test.rmse:           3.31
2025-09-02 21:13:22   metrics/test.rmse_pcutoff:   3.31
2025-09-02 21:13:22   metrics/test.mAP:          100.00
2025-09-02 21:13:22   metrics/test.mAR:          100.00
2025-09-02 21:13:23 Epoch 91/500 (lr=0.0001), train loss 0.00119
2025-09-02 21:13:25 Epoch 92/500 (lr=0.0001), train loss 0.00096
2025-09-02 21:13:26 Epoch 93/500 (lr=0.0001), train loss 0.00102
2025-09-02 21:13:28 Epoch 94/500 (lr=0.0001), train loss 0.00106
2025-09-02 21:13:29 Epoch 95/500 (lr=0.0001), train loss 0.00088
2025-09-02 21:13:30 Epoch 96/500 (lr=0.0001), train loss 0.00092
2025-09-02 21:13:32 Epoch 97/500 (lr=0.0001), train loss 0.00073
2025-09-02 21:13:33 Epoch 98/500 (lr=0.0001), train loss 0.00074
2025-09-02 21:13:35 Epoch 99/500 (lr=0.0001), train loss 0.00101
2025-09-02 21:13:36 Training for epoch 100 done, starting evaluation
2025-09-02 21:13:36 Epoch 100/500 (lr=0.0001), train loss 0.00090, valid loss 0.00112
2025-09-02 21:13:36 Model performance:
2025-09-02 21:13:36   metrics/test.rmse:           2.48
2025-09-02 21:13:36   metrics/test.rmse_pcutoff:   2.48
2025-09-02 21:13:36   metrics/test.mAP:          100.00
2025-09-02 21:13:36   metrics/test.mAR:          100.00
2025-09-02 21:13:37 Epoch 101/500 (lr=0.0001), train loss 0.00093
2025-09-02 21:13:39 Epoch 102/500 (lr=0.0001), train loss 0.00058
2025-09-02 21:13:40 Epoch 103/500 (lr=0.0001), train loss 0.00075
2025-09-02 21:13:42 Epoch 104/500 (lr=0.0001), train loss 0.00082
2025-09-02 21:13:43 Epoch 105/500 (lr=0.0001), train loss 0.00086
2025-09-02 21:13:45 Epoch 106/500 (lr=0.0001), train loss 0.00077
2025-09-02 21:13:46 Epoch 107/500 (lr=0.0001), train loss 0.00075
2025-09-02 21:13:47 Epoch 108/500 (lr=0.0001), train loss 0.00079
2025-09-02 21:13:49 Epoch 109/500 (lr=0.0001), train loss 0.00081
2025-09-02 21:13:50 Training for epoch 110 done, starting evaluation
2025-09-02 21:13:50 Epoch 110/500 (lr=0.0001), train loss 0.00082, valid loss 0.00107
2025-09-02 21:13:50 Model performance:
2025-09-02 21:13:50   metrics/test.rmse:           2.30
2025-09-02 21:13:50   metrics/test.rmse_pcutoff:   2.30
2025-09-02 21:13:50   metrics/test.mAP:          100.00
2025-09-02 21:13:50   metrics/test.mAR:          100.00
2025-09-02 21:13:52 Epoch 111/500 (lr=0.0001), train loss 0.00067
2025-09-02 21:13:53 Epoch 112/500 (lr=0.0001), train loss 0.00087
2025-09-02 21:13:54 Epoch 113/500 (lr=0.0001), train loss 0.00070
2025-09-02 21:13:56 Epoch 114/500 (lr=0.0001), train loss 0.00064
2025-09-02 21:13:57 Epoch 115/500 (lr=0.0001), train loss 0.00065
2025-09-02 21:13:59 Epoch 116/500 (lr=0.0001), train loss 0.00058
2025-09-02 21:14:00 Epoch 117/500 (lr=0.0001), train loss 0.00065
2025-09-02 21:14:01 Epoch 118/500 (lr=0.0001), train loss 0.00077
2025-09-02 21:14:03 Epoch 119/500 (lr=0.0001), train loss 0.00067
2025-09-02 21:14:04 Training for epoch 120 done, starting evaluation
2025-09-02 21:14:04 Epoch 120/500 (lr=1e-05), train loss 0.00078, valid loss 0.00104
2025-09-02 21:14:04 Model performance:
2025-09-02 21:14:04   metrics/test.rmse:           2.58
2025-09-02 21:14:04   metrics/test.rmse_pcutoff:   2.58
2025-09-02 21:14:04   metrics/test.mAP:          100.00
2025-09-02 21:14:04   metrics/test.mAR:          100.00
2025-09-02 21:14:05 Epoch 121/500 (lr=1e-05), train loss 0.00068
2025-09-02 21:14:07 Epoch 122/500 (lr=1e-05), train loss 0.00067
2025-09-02 21:14:08 Epoch 123/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:14:10 Epoch 124/500 (lr=1e-05), train loss 0.00070
2025-09-02 21:14:11 Epoch 125/500 (lr=1e-05), train loss 0.00092
2025-09-02 21:14:12 Epoch 126/500 (lr=1e-05), train loss 0.00075
2025-09-02 21:14:14 Epoch 127/500 (lr=1e-05), train loss 0.00066
2025-09-02 21:14:15 Epoch 128/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:14:17 Epoch 129/500 (lr=1e-05), train loss 0.00066
2025-09-02 21:14:18 Training for epoch 130 done, starting evaluation
2025-09-02 21:14:18 Epoch 130/500 (lr=1e-05), train loss 0.00068, valid loss 0.00099
2025-09-02 21:14:18 Model performance:
2025-09-02 21:14:18   metrics/test.rmse:           2.41
2025-09-02 21:14:18   metrics/test.rmse_pcutoff:   2.41
2025-09-02 21:14:18   metrics/test.mAP:          100.00
2025-09-02 21:14:18   metrics/test.mAR:          100.00
2025-09-02 21:14:19 Epoch 131/500 (lr=1e-05), train loss 0.00066
2025-09-02 21:14:21 Epoch 132/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:14:22 Epoch 133/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:14:23 Epoch 134/500 (lr=1e-05), train loss 0.00069
2025-09-02 21:14:25 Epoch 135/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:14:26 Epoch 136/500 (lr=1e-05), train loss 0.00068
2025-09-02 21:14:28 Epoch 137/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:14:29 Epoch 138/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:14:30 Epoch 139/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:14:32 Training for epoch 140 done, starting evaluation
2025-09-02 21:14:32 Epoch 140/500 (lr=1e-05), train loss 0.00063, valid loss 0.00105
2025-09-02 21:14:32 Model performance:
2025-09-02 21:14:32   metrics/test.rmse:           2.48
2025-09-02 21:14:32   metrics/test.rmse_pcutoff:   2.48
2025-09-02 21:14:32   metrics/test.mAP:          100.00
2025-09-02 21:14:32   metrics/test.mAR:          100.00
2025-09-02 21:14:33 Epoch 141/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:14:35 Epoch 142/500 (lr=1e-05), train loss 0.00071
2025-09-02 21:14:36 Epoch 143/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:14:37 Epoch 144/500 (lr=1e-05), train loss 0.00066
2025-09-02 21:14:39 Epoch 145/500 (lr=1e-05), train loss 0.00076
2025-09-02 21:14:40 Epoch 146/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:14:41 Epoch 147/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:14:43 Epoch 148/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:14:44 Epoch 149/500 (lr=1e-05), train loss 0.00072
2025-09-02 21:14:46 Training for epoch 150 done, starting evaluation
2025-09-02 21:14:46 Epoch 150/500 (lr=1e-05), train loss 0.00075, valid loss 0.00100
2025-09-02 21:14:46 Model performance:
2025-09-02 21:14:46   metrics/test.rmse:           2.37
2025-09-02 21:14:46   metrics/test.rmse_pcutoff:   2.37
2025-09-02 21:14:46   metrics/test.mAP:          100.00
2025-09-02 21:14:46   metrics/test.mAR:          100.00
2025-09-02 21:14:47 Epoch 151/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:14:48 Epoch 152/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:14:50 Epoch 153/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:14:51 Epoch 154/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:14:52 Epoch 155/500 (lr=1e-05), train loss 0.00045
2025-09-02 21:14:54 Epoch 156/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:14:55 Epoch 157/500 (lr=1e-05), train loss 0.00076
2025-09-02 21:14:57 Epoch 158/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:14:58 Epoch 159/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:14:59 Training for epoch 160 done, starting evaluation
2025-09-02 21:14:59 Epoch 160/500 (lr=1e-05), train loss 0.00059, valid loss 0.00099
2025-09-02 21:14:59 Model performance:
2025-09-02 21:14:59   metrics/test.rmse:           2.39
2025-09-02 21:14:59   metrics/test.rmse_pcutoff:   2.39
2025-09-02 21:14:59   metrics/test.mAP:          100.00
2025-09-02 21:14:59   metrics/test.mAR:          100.00
2025-09-02 21:15:01 Epoch 161/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:15:02 Epoch 162/500 (lr=1e-05), train loss 0.00067
2025-09-02 21:15:03 Epoch 163/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:15:05 Epoch 164/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:15:06 Epoch 165/500 (lr=1e-05), train loss 0.00068
2025-09-02 21:15:08 Epoch 166/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:15:09 Epoch 167/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:15:10 Epoch 168/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:15:12 Epoch 169/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:15:13 Training for epoch 170 done, starting evaluation
2025-09-02 21:15:13 Epoch 170/500 (lr=1e-05), train loss 0.00068, valid loss 0.00099
2025-09-02 21:15:13 Model performance:
2025-09-02 21:15:13   metrics/test.rmse:           2.34
2025-09-02 21:15:13   metrics/test.rmse_pcutoff:   2.34
2025-09-02 21:15:13   metrics/test.mAP:          100.00
2025-09-02 21:15:13   metrics/test.mAR:          100.00
2025-09-02 21:15:15 Epoch 171/500 (lr=1e-05), train loss 0.00068
2025-09-02 21:15:16 Epoch 172/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:15:17 Epoch 173/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:15:19 Epoch 174/500 (lr=1e-05), train loss 0.00067
2025-09-02 21:15:20 Epoch 175/500 (lr=1e-05), train loss 0.00079
2025-09-02 21:15:21 Epoch 176/500 (lr=1e-05), train loss 0.00070
2025-09-02 21:15:23 Epoch 177/500 (lr=1e-05), train loss 0.00071
2025-09-02 21:15:24 Epoch 178/500 (lr=1e-05), train loss 0.00069
2025-09-02 21:15:26 Epoch 179/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:15:27 Training for epoch 180 done, starting evaluation
2025-09-02 21:15:27 Epoch 180/500 (lr=1e-05), train loss 0.00060, valid loss 0.00099
2025-09-02 21:15:27 Model performance:
2025-09-02 21:15:27   metrics/test.rmse:           2.34
2025-09-02 21:15:27   metrics/test.rmse_pcutoff:   2.34
2025-09-02 21:15:27   metrics/test.mAP:          100.00
2025-09-02 21:15:27   metrics/test.mAR:          100.00
2025-09-02 21:15:28 Epoch 181/500 (lr=1e-05), train loss 0.00044
2025-09-02 21:15:30 Epoch 182/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:15:31 Epoch 183/500 (lr=1e-05), train loss 0.00073
2025-09-02 21:15:33 Epoch 184/500 (lr=1e-05), train loss 0.00067
2025-09-02 21:15:34 Epoch 185/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:15:35 Epoch 186/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:15:37 Epoch 187/500 (lr=1e-05), train loss 0.00071
2025-09-02 21:15:38 Epoch 188/500 (lr=1e-05), train loss 0.00067
2025-09-02 21:15:39 Epoch 189/500 (lr=1e-05), train loss 0.00066
2025-09-02 21:15:41 Training for epoch 190 done, starting evaluation
2025-09-02 21:15:41 Epoch 190/500 (lr=1e-05), train loss 0.00064, valid loss 0.00100
2025-09-02 21:15:41 Model performance:
2025-09-02 21:15:41   metrics/test.rmse:           2.38
2025-09-02 21:15:41   metrics/test.rmse_pcutoff:   2.38
2025-09-02 21:15:41   metrics/test.mAP:          100.00
2025-09-02 21:15:41   metrics/test.mAR:          100.00
2025-09-02 21:15:42 Epoch 191/500 (lr=1e-05), train loss 0.00081
2025-09-02 21:15:44 Epoch 192/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:15:45 Epoch 193/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:15:46 Epoch 194/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:15:48 Epoch 195/500 (lr=1e-05), train loss 0.00041
2025-09-02 21:15:49 Epoch 196/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:15:50 Epoch 197/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:15:52 Epoch 198/500 (lr=1e-05), train loss 0.00068
2025-09-02 21:15:53 Epoch 199/500 (lr=1e-05), train loss 0.00049
2025-09-02 21:15:55 Training for epoch 200 done, starting evaluation
2025-09-02 21:15:55 Epoch 200/500 (lr=1e-05), train loss 0.00080, valid loss 0.00097
2025-09-02 21:15:55 Model performance:
2025-09-02 21:15:55   metrics/test.rmse:           2.26
2025-09-02 21:15:55   metrics/test.rmse_pcutoff:   2.26
2025-09-02 21:15:55   metrics/test.mAP:          100.00
2025-09-02 21:15:55   metrics/test.mAR:          100.00
2025-09-02 21:15:56 Epoch 201/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:15:57 Epoch 202/500 (lr=1e-05), train loss 0.00072
2025-09-02 21:15:59 Epoch 203/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:16:00 Epoch 204/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:16:02 Epoch 205/500 (lr=1e-05), train loss 0.00071
2025-09-02 21:16:03 Epoch 206/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:16:04 Epoch 207/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:16:06 Epoch 208/500 (lr=1e-05), train loss 0.00077
2025-09-02 21:16:07 Epoch 209/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:16:08 Training for epoch 210 done, starting evaluation
2025-09-02 21:16:08 Epoch 210/500 (lr=1e-05), train loss 0.00065, valid loss 0.00099
2025-09-02 21:16:08 Model performance:
2025-09-02 21:16:08   metrics/test.rmse:           2.35
2025-09-02 21:16:08   metrics/test.rmse_pcutoff:   2.35
2025-09-02 21:16:08   metrics/test.mAP:          100.00
2025-09-02 21:16:08   metrics/test.mAR:          100.00
2025-09-02 21:16:10 Epoch 211/500 (lr=1e-05), train loss 0.00074
2025-09-02 21:16:11 Epoch 212/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:16:12 Epoch 213/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:16:14 Epoch 214/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:16:15 Epoch 215/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:16:17 Epoch 216/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:16:18 Epoch 217/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:16:19 Epoch 218/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:16:21 Epoch 219/500 (lr=1e-05), train loss 0.00068
2025-09-02 21:16:22 Training for epoch 220 done, starting evaluation
2025-09-02 21:16:22 Epoch 220/500 (lr=1e-05), train loss 0.00063, valid loss 0.00096
2025-09-02 21:16:22 Model performance:
2025-09-02 21:16:22   metrics/test.rmse:           2.27
2025-09-02 21:16:22   metrics/test.rmse_pcutoff:   2.27
2025-09-02 21:16:22   metrics/test.mAP:          100.00
2025-09-02 21:16:22   metrics/test.mAR:          100.00
2025-09-02 21:16:24 Epoch 221/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:16:25 Epoch 222/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:16:26 Epoch 223/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:16:28 Epoch 224/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:16:29 Epoch 225/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:16:30 Epoch 226/500 (lr=1e-05), train loss 0.00079
2025-09-02 21:16:32 Epoch 227/500 (lr=1e-05), train loss 0.00067
2025-09-02 21:16:33 Epoch 228/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:16:34 Epoch 229/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:16:36 Training for epoch 230 done, starting evaluation
2025-09-02 21:16:36 Epoch 230/500 (lr=1e-05), train loss 0.00062, valid loss 0.00100
2025-09-02 21:16:36 Model performance:
2025-09-02 21:16:36   metrics/test.rmse:           2.36
2025-09-02 21:16:36   metrics/test.rmse_pcutoff:   2.36
2025-09-02 21:16:36   metrics/test.mAP:          100.00
2025-09-02 21:16:36   metrics/test.mAR:          100.00
2025-09-02 21:16:37 Epoch 231/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:16:38 Epoch 232/500 (lr=1e-05), train loss 0.00044
2025-09-02 21:16:40 Epoch 233/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:16:41 Epoch 234/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:16:42 Epoch 235/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:16:44 Epoch 236/500 (lr=1e-05), train loss 0.00069
2025-09-02 21:16:45 Epoch 237/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:16:46 Epoch 238/500 (lr=1e-05), train loss 0.00079
2025-09-02 21:16:48 Epoch 239/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:16:49 Training for epoch 240 done, starting evaluation
2025-09-02 21:16:49 Epoch 240/500 (lr=1e-05), train loss 0.00066, valid loss 0.00100
2025-09-02 21:16:49 Model performance:
2025-09-02 21:16:49   metrics/test.rmse:           2.31
2025-09-02 21:16:49   metrics/test.rmse_pcutoff:   2.31
2025-09-02 21:16:49   metrics/test.mAP:          100.00
2025-09-02 21:16:49   metrics/test.mAR:          100.00
2025-09-02 21:16:50 Epoch 241/500 (lr=1e-05), train loss 0.00068
2025-09-02 21:16:52 Epoch 242/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:16:53 Epoch 243/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:16:55 Epoch 244/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:16:56 Epoch 245/500 (lr=1e-05), train loss 0.00073
2025-09-02 21:16:57 Epoch 246/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:16:59 Epoch 247/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:17:00 Epoch 248/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:17:01 Epoch 249/500 (lr=1e-05), train loss 0.00067
2025-09-02 21:17:03 Training for epoch 250 done, starting evaluation
2025-09-02 21:17:03 Epoch 250/500 (lr=1e-05), train loss 0.00070, valid loss 0.00101
2025-09-02 21:17:03 Model performance:
2025-09-02 21:17:03   metrics/test.rmse:           2.39
2025-09-02 21:17:03   metrics/test.rmse_pcutoff:   2.39
2025-09-02 21:17:03   metrics/test.mAP:          100.00
2025-09-02 21:17:03   metrics/test.mAR:          100.00
2025-09-02 21:17:04 Epoch 251/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:17:06 Epoch 252/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:17:07 Epoch 253/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:17:08 Epoch 254/500 (lr=1e-05), train loss 0.00081
2025-09-02 21:17:09 Epoch 255/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:17:11 Epoch 256/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:17:12 Epoch 257/500 (lr=1e-05), train loss 0.00071
2025-09-02 21:17:14 Epoch 258/500 (lr=1e-05), train loss 0.00067
2025-09-02 21:17:15 Epoch 259/500 (lr=1e-05), train loss 0.00068
2025-09-02 21:17:16 Training for epoch 260 done, starting evaluation
2025-09-02 21:17:16 Epoch 260/500 (lr=1e-05), train loss 0.00051, valid loss 0.00098
2025-09-02 21:17:16 Model performance:
2025-09-02 21:17:16   metrics/test.rmse:           2.33
2025-09-02 21:17:16   metrics/test.rmse_pcutoff:   2.33
2025-09-02 21:17:16   metrics/test.mAP:          100.00
2025-09-02 21:17:16   metrics/test.mAR:          100.00
2025-09-02 21:17:18 Epoch 261/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:17:19 Epoch 262/500 (lr=1e-05), train loss 0.00069
2025-09-02 21:17:20 Epoch 263/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:17:22 Epoch 264/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:17:23 Epoch 265/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:17:24 Epoch 266/500 (lr=1e-05), train loss 0.00047
2025-09-02 21:17:26 Epoch 267/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:17:27 Epoch 268/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:17:28 Epoch 269/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:17:30 Training for epoch 270 done, starting evaluation
2025-09-02 21:17:30 Epoch 270/500 (lr=1e-05), train loss 0.00051, valid loss 0.00098
2025-09-02 21:17:30 Model performance:
2025-09-02 21:17:30   metrics/test.rmse:           2.30
2025-09-02 21:17:30   metrics/test.rmse_pcutoff:   2.30
2025-09-02 21:17:30   metrics/test.mAP:          100.00
2025-09-02 21:17:30   metrics/test.mAR:          100.00
2025-09-02 21:17:31 Epoch 271/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:17:32 Epoch 272/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:17:34 Epoch 273/500 (lr=1e-05), train loss 0.00071
2025-09-02 21:17:35 Epoch 274/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:17:36 Epoch 275/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:17:38 Epoch 276/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:17:39 Epoch 277/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:17:40 Epoch 278/500 (lr=1e-05), train loss 0.00045
2025-09-02 21:17:42 Epoch 279/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:17:43 Training for epoch 280 done, starting evaluation
2025-09-02 21:17:43 Epoch 280/500 (lr=1e-05), train loss 0.00058, valid loss 0.00095
2025-09-02 21:17:43 Model performance:
2025-09-02 21:17:43   metrics/test.rmse:           2.25
2025-09-02 21:17:43   metrics/test.rmse_pcutoff:   2.25
2025-09-02 21:17:43   metrics/test.mAP:          100.00
2025-09-02 21:17:43   metrics/test.mAR:          100.00
2025-09-02 21:17:45 Epoch 281/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:17:46 Epoch 282/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:17:47 Epoch 283/500 (lr=1e-05), train loss 0.00069
2025-09-02 21:17:49 Epoch 284/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:17:50 Epoch 285/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:17:51 Epoch 286/500 (lr=1e-05), train loss 0.00049
2025-09-02 21:17:53 Epoch 287/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:17:54 Epoch 288/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:17:55 Epoch 289/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:17:57 Training for epoch 290 done, starting evaluation
2025-09-02 21:17:57 Epoch 290/500 (lr=1e-05), train loss 0.00053, valid loss 0.00095
2025-09-02 21:17:57 Model performance:
2025-09-02 21:17:57   metrics/test.rmse:           2.30
2025-09-02 21:17:57   metrics/test.rmse_pcutoff:   2.30
2025-09-02 21:17:57   metrics/test.mAP:          100.00
2025-09-02 21:17:57   metrics/test.mAR:          100.00
2025-09-02 21:17:59 Epoch 291/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:18:00 Epoch 292/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:18:02 Epoch 293/500 (lr=1e-05), train loss 0.00074
2025-09-02 21:18:03 Epoch 294/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:18:05 Epoch 295/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:18:06 Epoch 296/500 (lr=1e-05), train loss 0.00070
2025-09-02 21:18:08 Epoch 297/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:18:09 Epoch 298/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:18:11 Epoch 299/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:18:12 Training for epoch 300 done, starting evaluation
2025-09-02 21:18:12 Epoch 300/500 (lr=1e-05), train loss 0.00061, valid loss 0.00097
2025-09-02 21:18:12 Model performance:
2025-09-02 21:18:12   metrics/test.rmse:           2.33
2025-09-02 21:18:12   metrics/test.rmse_pcutoff:   2.33
2025-09-02 21:18:12   metrics/test.mAP:          100.00
2025-09-02 21:18:12   metrics/test.mAR:          100.00
2025-09-02 21:18:14 Epoch 301/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:18:15 Epoch 302/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:18:16 Epoch 303/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:18:18 Epoch 304/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:18:19 Epoch 305/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:18:20 Epoch 306/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:18:22 Epoch 307/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:18:23 Epoch 308/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:18:24 Epoch 309/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:18:26 Training for epoch 310 done, starting evaluation
2025-09-02 21:18:26 Epoch 310/500 (lr=1e-05), train loss 0.00061, valid loss 0.00099
2025-09-02 21:18:26 Model performance:
2025-09-02 21:18:26   metrics/test.rmse:           2.30
2025-09-02 21:18:26   metrics/test.rmse_pcutoff:   2.30
2025-09-02 21:18:26   metrics/test.mAP:          100.00
2025-09-02 21:18:26   metrics/test.mAR:          100.00
2025-09-02 21:18:27 Epoch 311/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:18:29 Epoch 312/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:18:30 Epoch 313/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:18:31 Epoch 314/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:18:33 Epoch 315/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:18:34 Epoch 316/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:18:35 Epoch 317/500 (lr=1e-05), train loss 0.00047
2025-09-02 21:18:37 Epoch 318/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:18:38 Epoch 319/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:18:39 Training for epoch 320 done, starting evaluation
2025-09-02 21:18:39 Epoch 320/500 (lr=1e-05), train loss 0.00051, valid loss 0.00100
2025-09-02 21:18:39 Model performance:
2025-09-02 21:18:39   metrics/test.rmse:           2.39
2025-09-02 21:18:39   metrics/test.rmse_pcutoff:   2.39
2025-09-02 21:18:39   metrics/test.mAP:          100.00
2025-09-02 21:18:39   metrics/test.mAR:          100.00
2025-09-02 21:18:41 Epoch 321/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:18:42 Epoch 322/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:18:44 Epoch 323/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:18:45 Epoch 324/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:18:46 Epoch 325/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:18:48 Epoch 326/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:18:49 Epoch 327/500 (lr=1e-05), train loss 0.00044
2025-09-02 21:18:50 Epoch 328/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:18:52 Epoch 329/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:18:53 Training for epoch 330 done, starting evaluation
2025-09-02 21:18:53 Epoch 330/500 (lr=1e-05), train loss 0.00059, valid loss 0.00100
2025-09-02 21:18:53 Model performance:
2025-09-02 21:18:53   metrics/test.rmse:           2.35
2025-09-02 21:18:53   metrics/test.rmse_pcutoff:   2.35
2025-09-02 21:18:53   metrics/test.mAP:          100.00
2025-09-02 21:18:53   metrics/test.mAR:          100.00
2025-09-02 21:18:55 Epoch 331/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:18:56 Epoch 332/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:18:57 Epoch 333/500 (lr=1e-05), train loss 0.00068
2025-09-02 21:18:59 Epoch 334/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:19:00 Epoch 335/500 (lr=1e-05), train loss 0.00047
2025-09-02 21:19:01 Epoch 336/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:19:03 Epoch 337/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:19:04 Epoch 338/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:19:05 Epoch 339/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:19:07 Training for epoch 340 done, starting evaluation
2025-09-02 21:19:07 Epoch 340/500 (lr=1e-05), train loss 0.00054, valid loss 0.00098
2025-09-02 21:19:07 Model performance:
2025-09-02 21:19:07   metrics/test.rmse:           2.17
2025-09-02 21:19:07   metrics/test.rmse_pcutoff:   2.17
2025-09-02 21:19:07   metrics/test.mAP:          100.00
2025-09-02 21:19:07   metrics/test.mAR:          100.00
2025-09-02 21:19:08 Epoch 341/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:19:10 Epoch 342/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:19:11 Epoch 343/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:19:12 Epoch 344/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:19:14 Epoch 345/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:19:15 Epoch 346/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:19:16 Epoch 347/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:19:18 Epoch 348/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:19:19 Epoch 349/500 (lr=1e-05), train loss 0.00043
2025-09-02 21:19:20 Training for epoch 350 done, starting evaluation
2025-09-02 21:19:21 Epoch 350/500 (lr=1e-05), train loss 0.00048, valid loss 0.00099
2025-09-02 21:19:21 Model performance:
2025-09-02 21:19:21   metrics/test.rmse:           2.29
2025-09-02 21:19:21   metrics/test.rmse_pcutoff:   2.29
2025-09-02 21:19:21   metrics/test.mAP:          100.00
2025-09-02 21:19:21   metrics/test.mAR:          100.00
2025-09-02 21:19:22 Epoch 351/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:19:23 Epoch 352/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:19:25 Epoch 353/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:19:26 Epoch 354/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:19:27 Epoch 355/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:19:29 Epoch 356/500 (lr=1e-05), train loss 0.00046
2025-09-02 21:19:30 Epoch 357/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:19:32 Epoch 358/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:19:33 Epoch 359/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:19:34 Training for epoch 360 done, starting evaluation
2025-09-02 21:19:34 Epoch 360/500 (lr=1e-05), train loss 0.00064, valid loss 0.00096
2025-09-02 21:19:34 Model performance:
2025-09-02 21:19:34   metrics/test.rmse:           2.25
2025-09-02 21:19:34   metrics/test.rmse_pcutoff:   2.25
2025-09-02 21:19:34   metrics/test.mAP:          100.00
2025-09-02 21:19:34   metrics/test.mAR:          100.00
2025-09-02 21:19:36 Epoch 361/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:19:37 Epoch 362/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:19:38 Epoch 363/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:19:40 Epoch 364/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:19:41 Epoch 365/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:19:43 Epoch 366/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:19:44 Epoch 367/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:19:45 Epoch 368/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:19:47 Epoch 369/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:19:48 Training for epoch 370 done, starting evaluation
2025-09-02 21:19:48 Epoch 370/500 (lr=1e-05), train loss 0.00080, valid loss 0.00097
2025-09-02 21:19:48 Model performance:
2025-09-02 21:19:48   metrics/test.rmse:           2.28
2025-09-02 21:19:48   metrics/test.rmse_pcutoff:   2.28
2025-09-02 21:19:48   metrics/test.mAP:          100.00
2025-09-02 21:19:48   metrics/test.mAR:          100.00
2025-09-02 21:19:50 Epoch 371/500 (lr=1e-05), train loss 0.00047
2025-09-02 21:19:51 Epoch 372/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:19:52 Epoch 373/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:19:54 Epoch 374/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:19:55 Epoch 375/500 (lr=1e-05), train loss 0.00047
2025-09-02 21:19:57 Epoch 376/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:19:58 Epoch 377/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:19:59 Epoch 378/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:20:01 Epoch 379/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:20:02 Training for epoch 380 done, starting evaluation
2025-09-02 21:20:02 Epoch 380/500 (lr=1e-05), train loss 0.00052, valid loss 0.00100
2025-09-02 21:20:02 Model performance:
2025-09-02 21:20:02   metrics/test.rmse:           2.29
2025-09-02 21:20:02   metrics/test.rmse_pcutoff:   2.29
2025-09-02 21:20:02   metrics/test.mAP:          100.00
2025-09-02 21:20:02   metrics/test.mAR:          100.00
2025-09-02 21:20:04 Epoch 381/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:20:05 Epoch 382/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:20:07 Epoch 383/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:20:08 Epoch 384/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:20:10 Epoch 385/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:20:11 Epoch 386/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:20:12 Epoch 387/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:20:14 Epoch 388/500 (lr=1e-05), train loss 0.00049
2025-09-02 21:20:15 Epoch 389/500 (lr=1e-05), train loss 0.00045
2025-09-02 21:20:16 Training for epoch 390 done, starting evaluation
2025-09-02 21:20:16 Epoch 390/500 (lr=1e-05), train loss 0.00065, valid loss 0.00098
2025-09-02 21:20:16 Model performance:
2025-09-02 21:20:16   metrics/test.rmse:           2.26
2025-09-02 21:20:16   metrics/test.rmse_pcutoff:   2.26
2025-09-02 21:20:16   metrics/test.mAP:          100.00
2025-09-02 21:20:16   metrics/test.mAR:          100.00
2025-09-02 21:20:18 Epoch 391/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:20:19 Epoch 392/500 (lr=1e-05), train loss 0.00066
2025-09-02 21:20:21 Epoch 393/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:20:22 Epoch 394/500 (lr=1e-05), train loss 0.00048
2025-09-02 21:20:23 Epoch 395/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:20:25 Epoch 396/500 (lr=1e-05), train loss 0.00073
2025-09-02 21:20:26 Epoch 397/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:20:28 Epoch 398/500 (lr=1e-05), train loss 0.00044
2025-09-02 21:20:29 Epoch 399/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:20:30 Training for epoch 400 done, starting evaluation
2025-09-02 21:20:30 Epoch 400/500 (lr=1e-05), train loss 0.00055, valid loss 0.00102
2025-09-02 21:20:30 Model performance:
2025-09-02 21:20:30   metrics/test.rmse:           2.34
2025-09-02 21:20:30   metrics/test.rmse_pcutoff:   2.34
2025-09-02 21:20:30   metrics/test.mAP:          100.00
2025-09-02 21:20:30   metrics/test.mAR:          100.00
2025-09-02 21:20:32 Epoch 401/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:20:33 Epoch 402/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:20:35 Epoch 403/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:20:36 Epoch 404/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:20:37 Epoch 405/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:20:39 Epoch 406/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:20:40 Epoch 407/500 (lr=1e-05), train loss 0.00046
2025-09-02 21:20:41 Epoch 408/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:20:43 Epoch 409/500 (lr=1e-05), train loss 0.00043
2025-09-02 21:20:44 Training for epoch 410 done, starting evaluation
2025-09-02 21:20:44 Epoch 410/500 (lr=1e-05), train loss 0.00055, valid loss 0.00101
2025-09-02 21:20:44 Model performance:
2025-09-02 21:20:44   metrics/test.rmse:           2.30
2025-09-02 21:20:44   metrics/test.rmse_pcutoff:   2.30
2025-09-02 21:20:44   metrics/test.mAP:          100.00
2025-09-02 21:20:44   metrics/test.mAR:          100.00
2025-09-02 21:20:46 Epoch 411/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:20:47 Epoch 412/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:20:49 Epoch 413/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:20:50 Epoch 414/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:20:51 Epoch 415/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:20:53 Epoch 416/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:20:54 Epoch 417/500 (lr=1e-05), train loss 0.00044
2025-09-02 21:20:55 Epoch 418/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:20:57 Epoch 419/500 (lr=1e-05), train loss 0.00049
2025-09-02 21:20:58 Training for epoch 420 done, starting evaluation
2025-09-02 21:20:58 Epoch 420/500 (lr=1e-05), train loss 0.00052, valid loss 0.00102
2025-09-02 21:20:58 Model performance:
2025-09-02 21:20:58   metrics/test.rmse:           2.41
2025-09-02 21:20:58   metrics/test.rmse_pcutoff:   2.41
2025-09-02 21:20:58   metrics/test.mAP:          100.00
2025-09-02 21:20:58   metrics/test.mAR:          100.00
2025-09-02 21:21:00 Epoch 421/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:21:01 Epoch 422/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:21:03 Epoch 423/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:21:05 Epoch 424/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:21:06 Epoch 425/500 (lr=1e-05), train loss 0.00048
2025-09-02 21:21:08 Epoch 426/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:21:09 Epoch 427/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:21:10 Epoch 428/500 (lr=1e-05), train loss 0.00046
2025-09-02 21:21:12 Epoch 429/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:21:13 Training for epoch 430 done, starting evaluation
2025-09-02 21:21:14 Epoch 430/500 (lr=1e-05), train loss 0.00050, valid loss 0.00097
2025-09-02 21:21:14 Model performance:
2025-09-02 21:21:14   metrics/test.rmse:           2.31
2025-09-02 21:21:14   metrics/test.rmse_pcutoff:   2.31
2025-09-02 21:21:14   metrics/test.mAP:          100.00
2025-09-02 21:21:14   metrics/test.mAR:          100.00
2025-09-02 21:21:15 Epoch 431/500 (lr=1e-05), train loss 0.00047
2025-09-02 21:21:16 Epoch 432/500 (lr=1e-05), train loss 0.00049
2025-09-02 21:21:18 Epoch 433/500 (lr=1e-05), train loss 0.00039
2025-09-02 21:21:19 Epoch 434/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:21:21 Epoch 435/500 (lr=1e-05), train loss 0.00046
2025-09-02 21:21:22 Epoch 436/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:21:24 Epoch 437/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:21:25 Epoch 438/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:21:27 Epoch 439/500 (lr=1e-05), train loss 0.00051
2025-09-02 21:21:28 Training for epoch 440 done, starting evaluation
2025-09-02 21:21:28 Epoch 440/500 (lr=1e-05), train loss 0.00054, valid loss 0.00099
2025-09-02 21:21:28 Model performance:
2025-09-02 21:21:28   metrics/test.rmse:           2.35
2025-09-02 21:21:28   metrics/test.rmse_pcutoff:   2.35
2025-09-02 21:21:28   metrics/test.mAP:          100.00
2025-09-02 21:21:28   metrics/test.mAR:          100.00
2025-09-02 21:21:30 Epoch 441/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:21:31 Epoch 442/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:21:33 Epoch 443/500 (lr=1e-05), train loss 0.00047
2025-09-02 21:21:34 Epoch 444/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:21:36 Epoch 445/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:21:37 Epoch 446/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:21:39 Epoch 447/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:21:40 Epoch 448/500 (lr=1e-05), train loss 0.00048
2025-09-02 21:21:41 Epoch 449/500 (lr=1e-05), train loss 0.00034
2025-09-02 21:21:43 Training for epoch 450 done, starting evaluation
2025-09-02 21:21:43 Epoch 450/500 (lr=1e-05), train loss 0.00066, valid loss 0.00097
2025-09-02 21:21:43 Model performance:
2025-09-02 21:21:43   metrics/test.rmse:           2.21
2025-09-02 21:21:43   metrics/test.rmse_pcutoff:   2.21
2025-09-02 21:21:43   metrics/test.mAP:          100.00
2025-09-02 21:21:43   metrics/test.mAR:          100.00
2025-09-02 21:21:44 Epoch 451/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:21:46 Epoch 452/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:21:47 Epoch 453/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:21:49 Epoch 454/500 (lr=1e-05), train loss 0.00049
2025-09-02 21:21:50 Epoch 455/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:21:52 Epoch 456/500 (lr=1e-05), train loss 0.00044
2025-09-02 21:21:53 Epoch 457/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:21:55 Epoch 458/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:21:56 Epoch 459/500 (lr=1e-05), train loss 0.00037
2025-09-02 21:21:57 Training for epoch 460 done, starting evaluation
2025-09-02 21:21:57 Epoch 460/500 (lr=1e-05), train loss 0.00048, valid loss 0.00097
2025-09-02 21:21:57 Model performance:
2025-09-02 21:21:57   metrics/test.rmse:           2.31
2025-09-02 21:21:57   metrics/test.rmse_pcutoff:   2.31
2025-09-02 21:21:57   metrics/test.mAP:          100.00
2025-09-02 21:21:57   metrics/test.mAR:          100.00
2025-09-02 21:21:59 Epoch 461/500 (lr=1e-05), train loss 0.00065
2025-09-02 21:22:00 Epoch 462/500 (lr=1e-05), train loss 0.00059
2025-09-02 21:22:02 Epoch 463/500 (lr=1e-05), train loss 0.00047
2025-09-02 21:22:03 Epoch 464/500 (lr=1e-05), train loss 0.00064
2025-09-02 21:22:05 Epoch 465/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:22:06 Epoch 466/500 (lr=1e-05), train loss 0.00049
2025-09-02 21:22:08 Epoch 467/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:22:09 Epoch 468/500 (lr=1e-05), train loss 0.00043
2025-09-02 21:22:11 Epoch 469/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:22:12 Training for epoch 470 done, starting evaluation
2025-09-02 21:22:12 Epoch 470/500 (lr=1e-05), train loss 0.00063, valid loss 0.00095
2025-09-02 21:22:12 Model performance:
2025-09-02 21:22:12   metrics/test.rmse:           2.25
2025-09-02 21:22:12   metrics/test.rmse_pcutoff:   2.25
2025-09-02 21:22:12   metrics/test.mAP:          100.00
2025-09-02 21:22:12   metrics/test.mAR:          100.00
2025-09-02 21:22:14 Epoch 471/500 (lr=1e-05), train loss 0.00048
2025-09-02 21:22:15 Epoch 472/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:22:17 Epoch 473/500 (lr=1e-05), train loss 0.00061
2025-09-02 21:22:18 Epoch 474/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:22:20 Epoch 475/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:22:21 Epoch 476/500 (lr=1e-05), train loss 0.00052
2025-09-02 21:22:23 Epoch 477/500 (lr=1e-05), train loss 0.00039
2025-09-02 21:22:24 Epoch 478/500 (lr=1e-05), train loss 0.00063
2025-09-02 21:22:26 Epoch 479/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:22:27 Training for epoch 480 done, starting evaluation
2025-09-02 21:22:27 Epoch 480/500 (lr=1e-05), train loss 0.00046, valid loss 0.00099
2025-09-02 21:22:27 Model performance:
2025-09-02 21:22:27   metrics/test.rmse:           2.21
2025-09-02 21:22:27   metrics/test.rmse_pcutoff:   2.21
2025-09-02 21:22:27   metrics/test.mAP:          100.00
2025-09-02 21:22:27   metrics/test.mAR:          100.00
2025-09-02 21:22:29 Epoch 481/500 (lr=1e-05), train loss 0.00060
2025-09-02 21:22:31 Epoch 482/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:22:32 Epoch 483/500 (lr=1e-05), train loss 0.00042
2025-09-02 21:22:34 Epoch 484/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:22:35 Epoch 485/500 (lr=1e-05), train loss 0.00047
2025-09-02 21:22:37 Epoch 486/500 (lr=1e-05), train loss 0.00053
2025-09-02 21:22:38 Epoch 487/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:22:39 Epoch 488/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:22:41 Epoch 489/500 (lr=1e-05), train loss 0.00045
2025-09-02 21:22:42 Training for epoch 490 done, starting evaluation
2025-09-02 21:22:42 Epoch 490/500 (lr=1e-05), train loss 0.00057, valid loss 0.00100
2025-09-02 21:22:42 Model performance:
2025-09-02 21:22:42   metrics/test.rmse:           2.28
2025-09-02 21:22:42   metrics/test.rmse_pcutoff:   2.28
2025-09-02 21:22:42   metrics/test.mAP:          100.00
2025-09-02 21:22:42   metrics/test.mAR:          100.00
2025-09-02 21:22:44 Epoch 491/500 (lr=1e-05), train loss 0.00054
2025-09-02 21:22:45 Epoch 492/500 (lr=1e-05), train loss 0.00049
2025-09-02 21:22:47 Epoch 493/500 (lr=1e-05), train loss 0.00062
2025-09-02 21:22:48 Epoch 494/500 (lr=1e-05), train loss 0.00058
2025-09-02 21:22:49 Epoch 495/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:22:51 Epoch 496/500 (lr=1e-05), train loss 0.00050
2025-09-02 21:22:52 Epoch 497/500 (lr=1e-05), train loss 0.00055
2025-09-02 21:22:53 Epoch 498/500 (lr=1e-05), train loss 0.00057
2025-09-02 21:22:55 Epoch 499/500 (lr=1e-05), train loss 0.00056
2025-09-02 21:22:56 Training for epoch 500 done, starting evaluation
2025-09-02 21:22:56 Epoch 500/500 (lr=1e-05), train loss 0.00050, valid loss 0.00099
2025-09-02 21:22:56 Model performance:
2025-09-02 21:22:56   metrics/test.rmse:           2.34
2025-09-02 21:22:56   metrics/test.rmse_pcutoff:   2.34
2025-09-02 21:22:56   metrics/test.mAP:          100.00
2025-09-02 21:22:56   metrics/test.mAR:          100.00
